# ğŸŒ RAG-Based Website Research Assistant

A Retrieval-Augmented Generation (RAG) powered web assistant that extracts content from websites and answers user queries using Large Language Models (LLMs).

---

## ğŸš€ Project Overview

This project allows users to:

- Input up to 3 website URLs
- Extract and process website content
- Store embeddings in a vector database (ChromaDB)
- Ask questions based only on the provided website content
- Get accurate, context-based answers with source references

The assistant ensures:
âœ” No hallucination  
âœ” Answers only from provided content  
âœ” Source transparency  

---

## ğŸ¯ Main Aim

To build an intelligent research assistant that:

- Automates website content analysis
- Uses RAG architecture for factual responses
- Avoids hallucinated outputs
- Provides source-backed answers

---

## ğŸ§  Basic Idea (How It Works)

1. User enters website URLs.
2. Website content is extracted using BeautifulSoup (via UnstructuredURLLoader).
3. Content is split into smaller chunks.
4. Chunks are converted into embeddings using HuggingFace models.
5. Embeddings are stored in ChromaDB.
6. When a user asks a question:
   - Relevant chunks are retrieved.
   - LLM (Groq - LLaMA model) generates answer strictly from retrieved content.
   - Sources are displayed.

---

## ğŸ—ï¸ Tech Stack

### ğŸ”¹ Framework
- LangChain

### ğŸ”¹ LLM
- Groq (LLaMA 3.3-70B Versatile - Free Version)

### ğŸ”¹ Embeddings
- HuggingFace (sentence-transformers/all-MiniLM-L6-v2)

### ğŸ”¹ Vector Database
- ChromaDB

### ğŸ”¹ Frontend
- Streamlit

### ğŸ”¹ Data Extraction
- BeautifulSoup (via UnstructuredURLLoader)

### ğŸ”¹ Environment Management
- Python-dotenv

---


